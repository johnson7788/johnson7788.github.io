<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>多标签分类的2种简单实现</title>
    <link href="/2022/02/28/multilabel/"/>
    <url>/2022/02/28/multilabel/</url>
    
    <content type="html"><![CDATA[<h1 id="模型的损失函数"><a href="#模型的损失函数" class="headerlink" title="模型的损失函数"></a>模型的损失函数</h1><p>torch.nn.BCEWithLogitsLoss<br>先进行了sigmoid，然后进行了二分类交叉熵损失函数</p><h1 id="评估metric"><a href="#评估metric" class="headerlink" title="评估metric"></a>评估metric</h1><p>sklearn.metrics.hamming_loss<br>sklearn.metrics.multilabel_confusion_matrix   #多标签混淆矩阵</p><h1 id="实现方式1：-使用CLS进行分类"><a href="#实现方式1：-使用CLS进行分类" class="headerlink" title="实现方式1： 使用CLS进行分类"></a>实现方式1： 使用CLS进行分类</h1><p>假设我们的示例是商品的购买意向，模型的基本输入是：CLS+句子1+SEP+商品+SEP<br>对CLS进行求sigmoid和二分类交叉熵<br>模型实现逻辑：</p><ol><li>Bert模型编码, last_hidden_state, all_hidden_states &#x3D; self.encode(input_ids, token_type_ids, attention_mask)</li><li>取最后一个隐藏层的CLS向量, first_token_tensor &#x3D; hidden_states[:, 0]  </li><li>进行dropout, self.dropout(first_token_tensor)</li><li>加个全连接层和激活</li><li>线性层映射到标签个数，得到logits, nn.Linear(hidden_size, num_labels)</li><li>计算损失, 反向传播,更新参数</li></ol><h1 id="实现方式2：-每个标签类别向量进行分类"><a href="#实现方式2：-每个标签类别向量进行分类" class="headerlink" title="实现方式2： 每个标签类别向量进行分类"></a>实现方式2： 每个标签类别向量进行分类</h1><p>假设我们的示例是商品的购买意向, 模型的基本输入是: CLS+句子1+SEP+商品+每个标签的id+SEP<br>取出每个标签的向量<br>对每个标签向量进行二分类交叉熵损失<br>模型实现逻辑：</p><ol><li>注意我们在输入的末尾添加了每个标签的id, 所以需要用特殊的token表示这些id, 所以需要增加词表，tokenizer.add_special_tokens({‘additional_special_tokens’:’opinion1’}) 我们用opinion1代表第一个标签，opinion2代表第二个标签，分别都加入到vocab中</li><li>注意在处理数据时，我们还有生成label_mask参数，告知我们关注的label的位置在哪里</li><li>Bert模型编码, last_hidden_state, all_hidden_states &#x3D; self.encode(input_ids, token_type_ids, attention_mask)</li><li>hidden_states 形状 [batch_size, seq_len, last_hidden_size] –&gt; [batch_size, labels_num, last_hidden_size]  加一个维度，然后扩充到hidden_states形状，方便后面取出label_mask需要的维度数据 label_mask_expand &#x3D; label_mask.unsqueeze(-1).expand(hidden_states.size())<br> labels_token_tensor_1d &#x3D; torch.masked_select(hidden_states, (label_mask_expand &#x3D;&#x3D; 1))<br>labels_token_tensor &#x3D; labels_token_tensor_1d.view(batch_size, -1, last_hidden_size)</li><li>加个droput, 全连接层和激活 </li><li>分类层, 最后变成1维度，nn.Linear(hidden_size, 1)</li><li>logits &#x3D; logits.squeeze(-1), 去掉最后一次，得到[batch_size, num_labels] 形状</li><li>计算损失, 反向传播,更新参数</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>一些论文中的专有名词</title>
    <link href="/2022/02/24/paper-word/"/>
    <url>/2022/02/24/paper-word/</url>
    
    <content type="html"><![CDATA[<h1 id="一些论文中的专有名词的解释或缩写"><a href="#一些论文中的专有名词的解释或缩写" class="headerlink" title="一些论文中的专有名词的解释或缩写"></a>一些论文中的专有名词的解释或缩写</h1><ul><li>low-resource:低资源:有标签或者无标签的训练的数据资源不足</li><li>Distant supervision:远端监督:大多数机器学习技术都需要一组训练数据。收集训练数据的传统方法是让人们标签一组文档。例如，对于婚姻关系，人类标注者可以将“比尔·克林顿”和“希拉里·克林顿”对标签为正的训练样本。这种方法在时间和金钱上都是昂贵的，并且如果我们的语料库很大，将无法产生足够的数据供我们的算法使用。而且由于人为错误，因此产生的训练数据极有可能是噪音的。生成训练数据的另一种方法是远距离监督(远程监督)。在远距离监督中，我们利用一个已经存在的数据库来收集要提取的关系的样本。然后，我们使用这些样本自动生成我们的训练数据。例如，包含巴拉克·奥巴马和米歇尔·奥巴马已婚的事实。我们考虑到这一事实，然后将出现在同一句子中的每对“巴拉克·奥巴马”和“米歇尔·奥巴马”都标签为我们婚姻关系的一个正例子。这样，我们可以轻松生成大量(可能是噪音的)训练数据。运用远距离监督来获得特定关系的正样本很容易，但是产生负样本比较难.即用知识库KG来获取2个实体之间的关系。</li><li>tokenization:分词器:原始raw text叫语料，字典中的单独词叫token，可能是单词，也可能是词语，取决于字典，tokenization是把raw text变成token的过程，假如英语就是把句子用空格切分，每个单词就叫token</li><li>detokenization:分词还原: 就是还原，把分词还原成句子，或者把分词后得到的id还原成原来的句子。</li><li>soft label:软标签:是一个teacher模型预测出来的，类似logits的概率值，是浮点数</li><li>hard label:硬标签:硬标签直接就是整数，就是对应概率最大的位置的索引，例如soft是0.82, hard就是1, <a href="https://arxiv.org/abs/1511.06335">https://arxiv.org/abs/1511.06335</a></li><li>SOTA:state-of-the-art:业界最新的性能，达到最新的模型性能</li><li>FLOPS:floating point operations per second:每秒浮动计算数, 是衡量计算机计算性能的一个指标</li><li>MLM:Masked language modeling, 掩盖语言建模, 也被叫做完形填空测试,cloze test,MLM的任务是根据占位符预测序列中的丢失token</li><li>T5: Text-to-Text Transfer Transformer</li><li>warm-up: 调整学习率的方式，在warm-up步数之前的学习率是恒定或者按照一定规则变大，warm-up之后的步数指数方式衰减和线性衰减</li><li>corruption损坏: 破坏一个原有的句子，例如BERT的MLM的无监督目标，可以对一个句子进行丢弃，替换，交换，添加操作，改变原有语句，然后让模型预测原有句子或改变的部分</li><li>域内数据: 就是一个领域的数据，例如新闻领域的文章和论文领域里的文章是不一样的,他们就是不同的域</li><li>pre-train-then-ﬁne-tune: 首先预训练模型，然后微调模型，预训练模型一般用大量数据做无监督训练，微调模型是用少量数据有监督训练</li><li>零样本学习(zero-shot learning): 即使训练时没有看到目标训练集，也能进行进行模型预测,零次训练或推理,无须训练，直接进行预测或推理。是一种训练策略，它允许机器学习模型预测新的类别，而不需要为新的类别提供任何标注的样本。</li><li>Few-Shot: 少量训练样本进行学习，然后预测，类似于low-resource</li><li>图灵完备性（Turing Completeness）:是针对一套数据操作规则而言的概念。数据操作规则可以是一门编程语言，也可以是计算机里具体实现了的指令集。当这套规则可以实现图灵机模型里的全部功能时，就称它具有图灵完备性。直白一点说，图灵完备性就是我给你一工具箱的东西，包括无限内存、if&#x2F;else 控制流、while 循环; 简单来讲，一切可计算的问题都能计算，这样的虚拟机或者编程语言就叫图灵完备的;举个例子，如果有人说，我的东西是图灵完备的，也就意味着理论上它能够用来解决任何计算性的问题。</li><li>有限状态机（英语：finite-state machine，缩写：FSM）又称有限状态自动机（英语：finite-state automation，缩写：FSA），简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学计算模型。</li><li>RE:relation extraction,neural relation extraction (NRE),  从一个句子中判断两个entity是否有关系，一般是一个二分类问题，指定某种关系。</li><li>Entity Mentions: 实体提及,就是句子中的实体, “New York City is good”  New York City就是实体，或者实体提及, 就是实体的名字</li><li>KGs: Knowledge Graph, 知识图谱</li><li>Autograd: 自动微分是训练神经网络的一种机制,自动求导，计算梯度</li><li>Segment: 片段，或者称为句子a，句子b等，例如训练BERT时结构如,“[CLS] x1 [SEP] x2 [SEP]”，x1表示片段1，或句子a，x2表示片段2或句子b。</li><li>Intrinsic tasks vs Downstream Tasks: 固有任务和下游任务，固有任务意思是预训练语言模型时的任务，下游任务是微调模型时的任务。</li><li>WordPiece: 是在自然语言处理中使用的子词分割算法。BERT用的此方法。子词分词的一种方法。 用该语言中的各个字符初始化单词表，然后将单词表中最常见的符号组合迭代添加到单词表中。 该过程是：1.用文本中的所有字符初始化单词清单。2.使用来自上一步的清单在训练数据上构建语言模型。 3. 通过组合当前单词清单中的两个单元, 将单词组装一个单词单元。 在添加到模型中时，从所有可能增加训练数据可能性中选择一个新的词单元。 4. 转到2，直到达到预定义的词单元限制或可能性增加低于某个特定阈值。</li><li>NMT:  Neural machine translation,神经机器翻译, 利用深度神经网络执行的端到端的翻译，例如seq2seq的神经网络翻译。</li><li>SMT: statistical machine translation,传统机器翻译的方法。</li><li>non-segmented语言: 分段语言，即用空格分隔的语言，例如英语，非分段语言，例如中文，日语，韩语。</li><li>NFD:Normalization Form Canonical Decomposition标准化形式规范分解,Unicode字符串标准化的一种算法,字符通过规范等价分解，并且多个组合字符按特定顺序排列</li><li>NFC:Normalization Form Canonical Composition 标准化形式规范组合,Unicode字符串标准化的一种算法, 字符被分解，然后通过规范对等重新组合。 </li><li>NFKD: Normalization Form Compatibility Decomposition: 标准化形式兼容性分解,字符通过兼容性分解，并且多个组合字符按特定顺序排列。 </li><li>NFKC: Normalization Form Compatibility Composition: 标准化形式兼容性组成,字符通过兼容性分解，然后通过规范对等重组。所有这些算法都是幂等转换，这意味着如果以相同算法再次处理，已经处于这些标准化形式之一的字符串将不会被修改。 </li><li>RBT3：由RoBERTa-wwm-ext 3层进行初始化，继续训练了1M步,RBT的名字是RoBERTa三个音节首字母组成，L代表large模型</li><li>RBTL3: 3层RoBERTa-wwm-ext-base&#x2F;large,由RoBERTa-wwm-ext-large 3层进行初始化，继续训练了1M步</li><li>ONNX: Open Neural Network Exchange format,开放式神经网络交换格式,提高模型推理速度的中间模型格式，最高实现4倍的推理加速。</li><li>ABSA: Aspect-based Sentiment Analysis, 给定句子中关心的情感的术语(aspect),即某个词在句子中表达的情感。等同于ALSC, Aspect level sentiment classification</li><li>Sentiment Analysis (SA): 也称为Opinion Mining (OM)</li><li>self-supervised: 类似于BERT的预训练模型的方式，也可以成为无监督,无监督表明我们确实没给BERT提供人工打标签，自监督表明它是用自己随机MASK部分token，然后预测被Mask的方式，所以叫做自监督。</li><li>context-gloss: 上下文的连贯性</li><li>LA-MLM: label-aware masked language model, 标签感知masked语言模型,分2个阶段Early Fusion早期融合和Late Supervision后期监督，它们的主要区别是早期融合阶段是把句子情感也作为输入，后期监督是把句子情感作为预测标签，监督训练句子情感。早期融合和后期监督的目的是让模型能够理解句子级情感和单词级情感和词性之间的内在联系。</li><li>parse tree: 分析树, 具体语法树（concrete syntax tree）,是一个反映某种形式语言字符串的语法关系的有根有序树。分析树一般按照两种相反的法则生成，一种是依存语法,一种是短语结构语法。二分类选举树,binary constituency tree</li><li>PLM: Pre-trained Language Models 预训练语言模型； 排列语言模型(Permutation Language Model) PLM, XLNet使用排列语言模型(PLM)</li><li>DRL: deep reinforcement learning</li><li>市场摩擦（英文：Market Friction）:是指金融资产在交易中存在的难度。它可由交易一定数量某金融资产的最佳占用时间来测定，也可由即时交易所需要的价格让步(Price concession)来测定。</li><li>inductive: 归纳式学习,transductive和inductive的区别在于我们想要预测的样本，是不是我们在训练的时候已经见（用）过的。inductive learning就是只根据现有的ABC，用比如kNN距离算法来预测，在来一个新的数据的时候，还是只根据5个ABC来预测。</li><li>transductive: 直推式学习,transductive learning直接以某种算法观察出数据的分布，这里呈现三个cluster，就根据cluster判定，不会建立一个预测的模型，如果一个新的数据加进来 就必须重新算一遍整个算法，新加的数据也会导致旧的已预测问号的结果改变</li><li>NRE: Neural Relation Extraction Models 神经网络的关系抽取模型</li><li>PCNN: PCNN（Piece-Wise-CNN）</li><li>OMR: 光学音乐识别(OPTICAL MUSIC Recognition，OMR)是将乐谱的扫描图像转换为像MusicXML[9]或MIDI这样的符号代表的问题。这种解决方案有很多明显的实际应用。</li><li>on-policy: 强化学习可以分为off-policy和on-policy的方法。off-policy RL算法意味着用于选择动作的行为策略与学习策略不同。相反，在on-policy RL算法中，行为策略与学习策略是相同的。此外，强化学习还可以分为基于价值的方法和基于策略的方法。在基于价值的RL中，agent更新价值函数来学习合适的策略，而基于策略的RL agent直接学习策略。</li><li>Hierarchical reinforcement learning (HRL): 分层强化学习</li><li>ALE: Atari Learning Environment</li><li>rollout:（就相当于在一个棋局时尝试多次不同路径的走子）类似右图产生多条路径</li><li>Imitation Learning: IL 模仿学习，模仿学习的思想很直观(intuitive)。我们在前面所介绍的Model-free, Model-based强化学习方法都是从零开始(from scratch)探索并学习一个使累计回报最大的策略(policy) [公式] 。 Imitation Learning的想法是，借助人类给出的示范(demonstration)，可以快速地达到这个目的。</li><li>• Forward model: (st, at) → st+1. 前向模型。(st, at) → st+1. 这是在给定当前状态和所选动作的情况下预测下一个状态。这是目前最常见的模型类型，可用于前向规划。</li><li>• Backward&#x2F;reverse model: st+1 → (st, at).  反向模型：st+1 →（st，at）。这个模型预测了哪些状态是某一特定状态的可能前兆。因此，我们可以在反向的方向上进行规划，例如，在prioritized sweeping中就使用了这种方法（Moore和Atkeson，1993）。</li><li>• Inverse model: (st, st+1) → at.  逆向模型。(st, st+1) → at. 逆向模型预测从一个状态到另一个状态需要哪种行动。例如，它被用于RRT规划中（LaValle，1998）。正如我们稍后将看到的那样，这个函数也可以作为表示学习的一部分。</li><li>NP-hard: NP是指非确定性多项式（non-deterministic polynomial，缩写NP）。所谓的非确定性是指，可用一定数量的运算去解决多项式时间内可解决的问题。例如，著名的推销员旅行问题（Travel Saleman Problem or TSP）：假设一个推销员需要从香港出发，经过广州，北京，上海，…，等 n 个城市， 最后返回香港。 任意两个城市之间都有飞机直达，但票价不等。假设公司只给报销 C 元钱，问是否存在一个行程安排，使得他能遍历所有城市，而且总的路费小于 C？ 推销员旅行问题显然是 NP 的。因为如果你任意给出一个行程安排，可以很容易算出旅行总开销。但是，要想知道一条总路费小于 C 的行程是否存在，在最坏情况下，必须检查所有可能的旅行安排！ 这将是个天文数字。</li><li>P类问题：可以找到一个多项式时间复杂度的算法去解决的问题；</li><li>NEXPTIME-complete:如果一个决策问题在NEXPTIME中，那么它就是NEXPTIME完整的，而且NEXPTIME中的每个问题都有一个多项式时间的多对一还原。换句话说，有一种多项式时间的算法可以将一个问题的实例转化为另一个问题的实例，而且答案相同。NEXPTIME-complete的问题可以被认为是NEXPTIME中最难的问题。我们知道NEXPTIME-complete问题不在NP中；根据时间层次定理，已经证明这些问题不能在多项式时间内被验证。 一组重要的NEXPTIME-complete问题与简洁电路有关。简明电路是一种简单的机器，用于在指数级的空间内描述图形。它们接受两个顶点数字作为输入，并输出它们之间是否有一条边。如果在自然表示法（如邻接矩阵）中解决一个图的问题是NP-完全的，那么在简洁电路表示法中解决同样的问题是NEXPTIME-完全的，因为输入是指数级的小（在一些温和的条件下，NP-完全性的减少是通过 “投影 “实现的）。[2][3] 作为一个简单的例子，为一个如此编码的图寻找一个汉密尔顿路径是NEXPTIME-完全的。</li><li>MARL: Multi-Agent Reinforcement Learning 多agent强化学习</li><li>annealed: 退火，意思是超参数随着时间的逐渐变小，参数越来越小。例如强化学习中的Qleanring的Greedy系数。</li><li>优势函数:advantage function , 强化学习 优势函数(Advantage Function),优势函数表达在状态s下，某动作a相对于平均而言的优势。 从数量关系来看，就是随机变量相对均值的偏差。 使用优势函数是深度强化学习极其重要的一种策略，尤其对于基于policy的学习。优势函数其实就是将Q-Value“归一化”到Value baseline上，如上讨论的，这样有助于提高学习效率，同时使学习更加稳定；同时经验表明，优势函数也有助于减小方差，而方差过大导致过拟合的重要因素。Aπ(s,a)&#x3D;Qπ(s,a) - Vπ(s)</li><li>bitext: bidirectional text, 双向语料，即翻译的平行语料，例如中英文翻译语料库</li><li>Dec-POMDP: 一个完全合作的多agent任务可以被描述为一个分散的部分可观察马尔可夫决策过程（Dec-POMDP）</li><li>CTDE: 集中训练和分散执行（CTDE）Kraemer和Banerjee[2016]的范式允许学习过程利用额外的状态信息。CTDE允许学习算法访问所有局部行动观察直方图和全局状态，并共享梯度和参数。然后，在执行阶段，每个单独的agent只能访问其局部行动观察历史τi。</li><li>tile-coding: 强化学习理论扩展到了连续空间（连续空间的泛化）。tile-coding是连续空间强化学习问题中最实用和计算效率最高的工具。本质上，tile-coding是连续状态空间中的特征表示,tile-coding的主要优势之一是其计算效率。一个tiling的意思是一张大网，里面有划分成不同的小网，观察的像素会落到这个tiling中的小网内，就可以用这个小网内的坐标表示这个特征，多个tiling就是用多个小网内的坐标表示这个特征，就像self-attention的多头和卷积中的多个卷积核一样。<a href="https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b">https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b</a></li><li>domain adaptation for machine translation (DAMT): 半监督领域适应翻译</li><li>ROUGE-N:系统和参考摘要之间N-grams的重叠。</li><li>ROUGE-1:指的是系统和参考摘要之间的unigram（每个字）的重叠情况。</li><li>ROUGE-2:指的是系统和参考摘要之间的bigram重叠。</li><li>ROUGE-L:基于最长共同子序列（LCS）的统计。最长共同子序列问题自然考虑到了句子层面的结构相似性，并自动识别序列中最长的共同出现的n-grams。</li><li>ROUGE-W:基于加权的LCS统计，倾向于连续的LCSs。</li><li>ROUGE-S:基于 Skip-bigram 的共现统计。Skip-bigram是指任何一对词在其句子中的序列。</li><li>ROUGE-SU:基于 Skip-bigram 和 unigram 的共现统计。</li><li>MTL: Multi-task learning </li><li>SAN: 随机答案网络,用于问答系统，stochastic answer network, Stochastic answer networks for natural language inference</li><li>MTPE:  machine translation post-editing</li><li>MBRL: model-based的强化学习</li><li>CPM:Chinese Pretrained language Model 中文预训练模型</li><li>prompt-tuning:（p-tuning）</li><li>TrGCN: transformer图卷积网络</li><li>NED:命名实体消歧named entity disambiguation</li><li>ERD: Entity Relationship Diagram 实体关系图</li><li>ELBO:  Evidence Lower Bound，即证据下界,这里的证据指数据或可观测变量的概率密度。使用变分推断时，首先需要计算的便是ELBO。<a href="https://blog.csdn.net/qy20115549/article/details/93074519">https://blog.csdn.net/qy20115549/article/details/93074519</a></li><li>New Words Discovery: 新词发现</li><li>ACSA: Aspect category sentiment analysis, 基于属性类别的情感分析</li><li>ABSA: Aspect Based Sentiment Analysis, 基于属性的情感分析</li><li>ATSA: 等同于ABSA，aspect术语情感分析</li><li>capsule Network: 胶囊神经网络,是相对于CNN的改进，综合了CNN的优点的同时，考虑了CNN缺失的相对位置、角度等其他信息，从而使得识别效果有所提升。<a href="https://easyai.tech/ai-definition/capsule/">https://easyai.tech/ai-definition/capsule/</a></li><li>MRR: Mean Reciprocal Rank, 是一个国际上通用的对 搜索算法 进行评价的机制，即第一个结果匹配，分数为1，第二个匹配分数为0.5，第n个匹配分数为1&#x2F;n，如果没有匹配的句子分数为0,最终的分数为所有得分之和</li><li>ER: entity recognition 实体识别</li><li>EL: entity linking 实体链接</li><li>ED: entity disambiguation 实体消歧</li><li>EEs: emerging entities 新出现的实体</li><li>EED: emerging entity discovery 新实体发现,通过对知识库中现有的候选实体进行鉴别,KB外的实体称为新出现的实体。</li><li>BPR: Best Possible Recall</li><li>CMD: Central Moment Discrepancy, 最先进的分布相似度指标,过匹配两个表示的顺序矩差来衡量它们之间的差异,类似KL散度,可以执行高阶矩的明确匹配，而不需要昂贵的距离和核矩阵计算。CMD距离随着两个分布的相似性而变小。</li><li>平凡解: trivial solution, 编码器函数近似于一个不相关的但不具代表性的模态向量，就会出现平凡解的情况，防止平凡解, 经常用于结构非常简单的对象（比如群或拓扑空间），有时亦会用明显或乏趣这两个词代替，但对非数学工作者来说，它们有时可能比其他更复杂的对象更难想象或理解。例如： 明显因数：对于每个正整数 n 来说，1、-1、n 和 -n 都是它的明显因数。 空集：不包含任何元素的集合； 平凡群：只含单位元的群；</li><li>DETR: 端到端目标检测,End-to-end Object Detection with Transformer, 达到与Faster-RCNN等两阶段目标检测相当的性能。</li><li>FGM: Factor Graph Model, 因子图模型,是概率图模型的一种</li><li>IGL: Iterative Grid Labeling 迭代式网关标注方式</li><li>TSA: Text Similarity Approach 文本相似性技术</li><li>MLTC:multi-label text classification 多标签文本分类</li><li>XMC: Extreme multi-label text classification 极端多标签文本分类, 寻求从一个极端大的标签集合中为给定的文本输入找到相关的标签。许多现实世界的应用可以被表述为XMC问题，如推荐系统、文档tagging和语义搜索。</li><li>exposure bias: 暴露偏差:模型训练与预测过程之间的不匹配。在训练时每一个词输入都来自真实样本，但是在推断时当前输入用的却是上一个词的输入</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>知识图谱的构建思路</title>
    <link href="/2022/02/24/knowledge-graph/"/>
    <url>/2022/02/24/knowledge-graph/</url>
    
    <content type="html"><![CDATA[<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><ol><li>众所周知，计算机的计算能力比人类强，但是推理能力远不如人类，所以构建知识图谱是为了解决计算机的推理能力。。</li><li>计算机的常识的使用能力也远不如人类，所以经常可以看到构建常识的知识图谱。</li><li>知识图谱在专业领域一般应用较多，例如医疗行业等。</li><li>知识图谱分为构建和应用，其中构建又包括设计本体，知识获取（结构化，非结构化，半结构化）知识存储，知识表示和知识融合等。如今构建知识图谱的技术已经很成熟，部分应用知识图谱的技术也已经成熟，例如利用知识图谱的问题等。</li></ol><img src="/2022/02/24/knowledge-graph/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" class="">]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>torch和paddlepaddle的部分API对比</title>
    <link href="/2022/02/23/torch-paddle/"/>
    <url>/2022/02/23/torch-paddle/</url>
    
    <content type="html"><![CDATA[<h1 id="paddlepaddle代码和torch的模型代码相互转换-其实只需要关注这些api的不同，进行相应替换即可"><a href="#paddlepaddle代码和torch的模型代码相互转换-其实只需要关注这些api的不同，进行相应替换即可" class="headerlink" title="paddlepaddle代码和torch的模型代码相互转换, 其实只需要关注这些api的不同，进行相应替换即可"></a>paddlepaddle代码和torch的模型代码相互转换, 其实只需要关注这些api的不同，进行相应替换即可</h1><p>paddlepaddle 封装了类似torch， huggingface transformers 和datasets的接口形式</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-title">torch</span>包和paddle包的对比, 只列出不同的地方<br><span class="hljs-type">PyTorch</span> <span class="hljs-type">PaddlePaddle</span>    说明<br><span class="hljs-title">torch</span>.nn    paddle.nn   包括了神经网络相关的大部分函数<br><span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>   nn.<span class="hljs-type">Layer</span>    搭建网络时集成的父类，包含了初始化等基本功能<br><span class="hljs-title">torch</span>.optim paddle.optimizer    训练优化器<br><span class="hljs-title">torch</span>.optim.<span class="hljs-type">AdamW</span>   paddle.optimizer.<span class="hljs-type">AdamW</span>  参数也不一样<br><span class="hljs-title">torchvision</span>.transforms  paddle.vision.transforms    数据预处理、图片处理<br><span class="hljs-title">torchvision</span>.datasets    paddle.vision.datasets  数据集的加载与处理<br><span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>   nn.<span class="hljs-type">Conv2D</span>   <span class="hljs-number">2</span>维卷积层<br><span class="hljs-title">nn</span>.<span class="hljs-type">BatchNorm2d</span>  nn.<span class="hljs-type">BatchNorm2D</span>  <span class="hljs-type">Batch</span> <span class="hljs-type">Normalization</span> 归一化<br><span class="hljs-title">nn</span>.<span class="hljs-type">MaxPool2d</span>    nn.<span class="hljs-type">MaxPool2D</span>    二维最大池化层<br><span class="hljs-title">nn</span>.<span class="hljs-type">AdaptiveAvgPool2d</span>    nn.<span class="hljs-type">AdaptiveAvgPool2D</span>    自适应二维平均池化（只用给定输出形状即可）<br><span class="hljs-title">torch</span>.flatten   paddle.flatten  展平处理<br><span class="hljs-title">torch</span>.softmax   paddle.softmax  softmax层<br><span class="hljs-title">datasets</span>.<span class="hljs-type">ImageFolder</span>    datasets.<span class="hljs-type">DatasetFolder</span>  指定数据集文件夹<br><span class="hljs-title">torch</span>.utils.<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">DataLoader</span> paddle.io.<span class="hljs-type">DataLoader</span>    加载数据集, 参数也不一样</span><br>(optimizer).no_grad (optimizer).zero_grad   梯度清零<br><span class="hljs-title">torch</span>.save  paddle.jit.save 说实话，这两个还是有点区别的，使用请看官方文档<br><span class="hljs-title">torch</span>.device    paddle.set_device   指定设备<br><span class="hljs-title">torch</span>.utils.<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">Dataset</span>    paddle.io.<span class="hljs-type">Dataset</span>   数据集</span><br><span class="hljs-title">torch</span>.utils.<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">RandomSampler</span>  paddle.io.<span class="hljs-type">RandomSampler</span></span><br><span class="hljs-title">torch</span>.utils.<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">BatchSampler</span>   paddle.io.<span class="hljs-type">BatchSampler</span>   参数也不一样</span><br><span class="hljs-title">torch</span>.utils.<span class="hljs-class"><span class="hljs-keyword">data</span>.<span class="hljs-type">DataLoader</span> paddle.io.<span class="hljs-type">DataLoader</span>  数据集加载</span><br><span class="hljs-title">tensor</span>.<span class="hljs-class"><span class="hljs-keyword">type</span>(<span class="hljs-title">torch</span>.<span class="hljs-title">float32</span>)  paddle.cast(<span class="hljs-title">mask</span>, &#x27;<span class="hljs-title">float32&#x27;</span>)  数据类型变更</span><br><span class="hljs-title">tensor</span>.cpu().item() tensor.numpy().item()  # 取出数据<br><span class="hljs-title">transformers</span>.get_linear_schedule_with_warmup    paddlenlp.transformers.<span class="hljs-type">LinearDecayWithWarmup</span>   # warmup 函数<br>也不一样<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>NLP项目和论文搜索</title>
    <link href="/2022/02/22/NLP%E9%A1%B9%E7%9B%AE%E5%92%8C%E8%AE%BA%E6%96%87%E6%90%9C%E7%B4%A2/"/>
    <url>/2022/02/22/NLP%E9%A1%B9%E7%9B%AE%E5%92%8C%E8%AE%BA%E6%96%87%E6%90%9C%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="NLP项目的一般解决思路"><a href="#NLP项目的一般解决思路" class="headerlink" title="NLP项目的一般解决思路"></a>NLP项目的一般解决思路</h1><img src="/2022/02/22/NLP%E9%A1%B9%E7%9B%AE%E5%92%8C%E8%AE%BA%E6%96%87%E6%90%9C%E7%B4%A2/%E5%A6%82%E4%BD%95%E5%81%9A%E4%B8%80%E4%B8%AAAI%E9%A1%B9%E7%9B%AE.png" class="">]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>一个小的子列表位置查找函数</title>
    <link href="/2022/02/22/python-list/"/>
    <url>/2022/02/22/python-list/</url>
    
    <content type="html"><![CDATA[<h1 id="sublist是original列表中的一部分，即子列表，如果存在sublist，那么就返回起始和结束位置，否则返回None"><a href="#sublist是original列表中的一部分，即子列表，如果存在sublist，那么就返回起始和结束位置，否则返回None" class="headerlink" title="sublist是original列表中的一部分，即子列表，如果存在sublist，那么就返回起始和结束位置，否则返回None"></a>sublist是original列表中的一部分，即子列表，如果存在sublist，那么就返回起始和结束位置，否则返回None</h1><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs vim">def index_partof_list(original, sublist):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-comment">&quot;</span><br>    子列表查找, 也可以用于字符串的查找<br>    :param original: 一个列表<br>    :<span class="hljs-built_in">type</span> original:<br>    :param sublist: sublist是original列表中的一部分，即子列表，如果存在sublist，那么就返回起始和结束位置，否<br>则返回None<br>    :<span class="hljs-built_in">type</span> sublist:<br>    :<span class="hljs-keyword">return</span>: 返回<span class="hljs-number">2</span>个值，bool值和查找到的索引值，如果没查找到返回[] 如果找到一个或找到多个，返回 [(x1,y1),(x2,y2)]<br>    :rtype:<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-comment">&quot;</span><br>    ori_len = <span class="hljs-built_in">len</span>(original)<br>    sub_len = <span class="hljs-built_in">len</span>(sublist)<br>    find_indexes = []<br>    <span class="hljs-keyword">if</span> ori_len &lt; sub_len:<br>        <span class="hljs-keyword">return</span> find_indexes<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">index</span> in <span class="hljs-built_in">range</span>(ori_len-sub_len+<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">if</span> original[<span class="hljs-built_in">index</span>:<span class="hljs-built_in">index</span> + sub_len] == sublist:<br>            find_indexes.<span class="hljs-keyword">append</span>((<span class="hljs-built_in">index</span>, <span class="hljs-built_in">index</span>+ sub_len))<br>    <span class="hljs-keyword">return</span> find_indexes<br><br>ori = [<span class="hljs-string">&#x27;《&#x27;</span>, <span class="hljs-string">&#x27;邪&#x27;</span>, <span class="hljs-string">&#x27;少&#x27;</span>, <span class="hljs-string">&#x27;兵&#x27;</span>, <span class="hljs-string">&#x27;王&#x27;</span>, <span class="hljs-string">&#x27;》&#x27;</span>, <span class="hljs-string">&#x27;是&#x27;</span>, <span class="hljs-string">&#x27;冰&#x27;</span>, <span class="hljs-string">&#x27;火&#x27;</span>, <span class="hljs-string">&#x27;未&#x27;</span>, <span class="hljs-string">&#x27;央&#x27;</span>, <span class="hljs-string">&#x27;写&#x27;</span>, <span class="hljs-string">&#x27;的&#x27;</span>, <span class="hljs-string">&#x27;网&#x27;</span>, <span class="hljs-string">&#x27;络&#x27;</span>, <span class="hljs-string">&#x27;小&#x27;</span>, <span class="hljs-string">&#x27;说&#x27;</span>, <span class="hljs-string">&#x27;连&#x27;</span>, <span class="hljs-string">&#x27;载&#x27;</span>, <span class="hljs-string">&#x27;于&#x27;</span>, <span class="hljs-string">&#x27;旗&#x27;</span>, <span class="hljs-string">&#x27;峰&#x27;</span>, <span class="hljs-string">&#x27;天&#x27;</span>, <span class="hljs-string">&#x27;下&#x27;</span>]<br>sub = [<span class="hljs-string">&#x27;网&#x27;</span>, <span class="hljs-string">&#x27;络&#x27;</span>, <span class="hljs-string">&#x27;小&#x27;</span>, <span class="hljs-string">&#x27;说&#x27;</span>]<br><span class="hljs-keyword">res</span> = index_partof_list(ori,sub)<br><span class="hljs-keyword">print</span>(<span class="hljs-keyword">res</span>)<br>------&gt;<br>[(<span class="hljs-number">13</span>, <span class="hljs-number">17</span>)]<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>对存在过合并的excel的单元格进行处理</title>
    <link href="/2022/02/22/pandas-na/"/>
    <url>/2022/02/22/pandas-na/</url>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>日常我们经常遇到表头是合并的单元格，如左侧表头，或者上测表头都是合并过的，而我们想读取使用pandas读取的excel后，每列都进行对应回原来的数据的结构，那么这时就需要进行填充了，因为读取后，只有合并的单元格的第一行或第一列是有值的，其它都是nan，我们需要用前向填充的方法，ffill()<br>示例如图:</p><img src="/2022/02/22/pandas-na/shili1.png" class=""><h1 id="填充代码，可以给定超参数，填充表头，按行和按列填充"><a href="#填充代码，可以给定超参数，填充表头，按行和按列填充" class="headerlink" title="填充代码，可以给定超参数，填充表头，按行和按列填充"></a>填充代码，可以给定超参数，填充表头，按行和按列填充</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">fill_pdna</span>(<span class="hljs-params">df, row=[], col=[]</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    当excel的表头的行或列有合并单元格的情况时，只有第一个单元格是正确的，值，这时候需要使用前向填充ffill，即使用上一个单元格的内容填充当前为nan的单元格</span><br><span class="hljs-string">    但是填充的时候一般进行限制，只填充表头的前几行，或前几列</span><br><span class="hljs-string">    :param df:</span><br><span class="hljs-string">    :type df:</span><br><span class="hljs-string">    :param row: [] 表示所有行都使用前面的值进行填充，1表示第一行, eg: [1,2] 表示第1，2行用前面的值填充,-1表示不填充</span><br><span class="hljs-string">    :param col: []表示，所有列都使用前面的值填充， 0表示第一列, 注意行和列的其实索引位置不一样, -1表示不填充</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    :rtype:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 首先对行进行填充，填充哪些行</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> row:<br>        <span class="hljs-comment"># 如果为空，先按行进行填充，行空的时候使用前一个单元格填充</span><br>        df = df.ffill(axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> col:<br>        <span class="hljs-comment"># 然后对列进行填充</span><br>        df = df.ffill(axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">if</span> col <span class="hljs-keyword">and</span> col != [-<span class="hljs-number">1</span>]:<br>        <span class="hljs-keyword">for</span> col_num <span class="hljs-keyword">in</span> col:<br>            df[col_num] = df[col_num].ffill(axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">if</span> row <span class="hljs-keyword">and</span> row != [-<span class="hljs-number">1</span>]:<br>        <span class="hljs-keyword">for</span> row_num <span class="hljs-keyword">in</span> row:<br>            df[:row_num] = df[:row_num].ffill(axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> df<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_excel</span>(<span class="hljs-params">excel_file</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    读取excel内容</span><br><span class="hljs-string">    :param excel_file:</span><br><span class="hljs-string">    :type excel_file:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    :rtype:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;开始读取<span class="hljs-subst">&#123;excel_file&#125;</span>&quot;</span>)<br>    df = pd.read_excel(excel_file, header=<span class="hljs-literal">None</span>)<br>    newdf = fill_pdna(df, row=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], col=[<span class="hljs-number">0</span>])<br>    <span class="hljs-built_in">print</span>(newdf)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>shap值的探索，判断shap值是否符合基本单调递增</title>
    <link href="/2022/02/18/shap-explore2/"/>
    <url>/2022/02/18/shap-explore2/</url>
    
    <content type="html"><![CDATA[<h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><ol><li>构建一个模型，本示例用的XGBoost，然后构建shap解释模型，使用shap值对模型特征进行解释</li><li>按特征重要性，这里对应的是shap值的绝对值的均值，shap_values.abs.mean，进行排序</li><li>如果特征符合基本单调递增, 不一定是线性的，因为特征之间可能有相关性,打印对应shap值为0附近的原始特征数值，用原始特征的均值代替</li></ol><h1 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h1><p>我们构建的是一个客户满意度模型，使用的是用户对一个商品的整体满意度与商品的各个属性满意度之间的关系，我们想找出当每个属性的满意度达到多少时，才能对整体满意度产生影响，即各个属性满意度的理想值。</p><p>示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> xgboost<br><span class="hljs-keyword">import</span> shap<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br>mpl.rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<br>mpl.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><br><br>saved_file = <span class="hljs-string">&#x27;/tmp/adult.pkl&#x27;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dump_info</span>(<span class="hljs-params">data</span>):<br>    pickle.dump(data, <span class="hljs-built_in">open</span>(saved_file, <span class="hljs-string">&quot;wb&quot;</span>))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;保存成功&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_info</span>():<br>    data = pickle.load(<span class="hljs-built_in">open</span>(saved_file, <span class="hljs-string">&quot;rb&quot;</span>))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;加载成功&quot;</span>)<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-keyword">if</span> os.path.exists(saved_file):<br>    X,y = load_info()<br><span class="hljs-keyword">else</span>:<br>    X,y = shap.datasets.adult()<br>    dump_info(data=(X,y))<br>model = xgboost.XGBClassifier().fit(X, y)<br><br><span class="hljs-comment"># compute SHAP values</span><br>explainer = shap.Explainer(model, X)<br>shap_values = explainer(X)<br><span class="hljs-comment"># shap_values [num_samples, num_features]</span><br><span class="hljs-comment"># shap.plots.beeswarm(shap_values)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_closely_sublist</span>(<span class="hljs-params">src_list, percent=<span class="hljs-number">0.05</span>, des_num=<span class="hljs-number">0.3</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    找出src_list 中与des_num最接近的数字，找到总数量的为百分之percent</span><br><span class="hljs-string">    :param src_list:</span><br><span class="hljs-string">    :type src_list: list</span><br><span class="hljs-string">    :param percent:</span><br><span class="hljs-string">    :type percent:</span><br><span class="hljs-string">    :param des_num:</span><br><span class="hljs-string">    :type des_num:</span><br><span class="hljs-string">    :return: 返回百分之percent的数据的个数的列表，列表是src_list的子列表</span><br><span class="hljs-string">    :rtype:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 取值为0范围的%5的数</span><br>    total_num = <span class="hljs-built_in">len</span>(src_list)<br>    got_num = <span class="hljs-built_in">int</span>(total_num * percent)<br>    left_num = right_num = <span class="hljs-built_in">int</span>(got_num/<span class="hljs-number">2</span>)<br>    sorted_l = <span class="hljs-built_in">sorted</span>(src_list)<br>    <span class="hljs-comment">#定位与0最接近的位置的索引</span><br>    min_closest_idx = <span class="hljs-number">0</span><br>    min_closed_distance = <span class="hljs-number">100000</span><br>    <span class="hljs-keyword">for</span> idx, i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sorted_l):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(i - des_num) &lt; min_closed_distance:<br>            min_closed_distance = <span class="hljs-built_in">abs</span>(i - des_num)<br>            min_closest_idx = idx<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;最接近于<span class="hljs-subst">&#123;des_num&#125;</span>的数字是<span class="hljs-subst">&#123;sorted_l[min_closest_idx]&#125;</span>&quot;</span>)<br>    start_idx = min_closest_idx - left_num<br>    <span class="hljs-keyword">if</span> start_idx &lt; <span class="hljs-number">0</span>:<br>        start_idx = <span class="hljs-number">0</span><br>    end_idx = min_closest_idx + right_num<br>    sublist = sorted_l[start_idx:end_idx]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;收集接近于目标值<span class="hljs-subst">&#123;des_num&#125;</span>, 总数据条数:<span class="hljs-subst">&#123;total_num&#125;</span>, 收集占比为<span class="hljs-subst">&#123;percent&#125;</span>,共收集到数据条数: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(sublist)&#125;</span>条，分别是: <span class="hljs-subst">&#123;sublist&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> sublist<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_middle_data</span>(<span class="hljs-params">mean_shape</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据给定的shap，获取shap值为0时，原始data的值，因为有的值不是单递增的，还要判断是否是单调递增的， 统计的方法判断</span><br><span class="hljs-string">    根据均值和中位数，判断是否是单调递增的，大部分不是线性递增的</span><br><span class="hljs-string">    :param mean_shape:</span><br><span class="hljs-string">    :type mean_shape:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    :rtype:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    feature_name = mean_shape.feature_names<br>    shape_value = mean_shape.values<br>    feature_data = mean_shape.data<br>    <span class="hljs-comment"># 按大小排序</span><br>    sort_shap = np.sort(shape_value)<br>    sort_shap = sort_shap.tolist()<br>    <span class="hljs-comment"># 最接近0的shap值，大概5%</span><br>    sublist = find_closely_sublist(src_list=sort_shap,percent=<span class="hljs-number">0.05</span>, des_num=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># 取0轴为的5%的数</span><br>    start_threhold, end_threhold = <span class="hljs-built_in">min</span>(sublist), <span class="hljs-built_in">max</span>(sublist)<br>    zero_range_shap_idx = np.where((shape_value &gt;= start_threhold) &amp; (shape_value &lt;= end_threhold))<br>    <span class="hljs-comment">#判断是否单调的问题，大部分shap值小于zero附近shap的，它对应的原始特征数也小于，shap值大于zero附近的，它的原始特征对应的数据也大于zero的原始特征，咱们都用平均值和中位数2个结合判断</span><br>    zero_range_data = feature_data[zero_range_shap_idx]<br>    zero_data_mean = np.mean(zero_range_data)<br>    zero_data_median = np.median(zero_range_data)<br>    less_zero_shap_idx = np.where(shape_value &lt; start_threhold)<br>    biger_zero_shap_idx = np.where(shape_value &gt; end_threhold)<br>    less_zero_data = feature_data[less_zero_shap_idx]<br>    biger_zero_data = feature_data[biger_zero_shap_idx]<br>    less_zero_mean = np.mean(less_zero_data)<br>    less_zero_median = np.median(less_zero_data)<br>    biger_zero_mean = np.mean(biger_zero_data)<br>    biger_zero_median = np.median(biger_zero_data)<br>    <span class="hljs-keyword">if</span> less_zero_mean &lt; zero_data_mean &lt; biger_zero_mean <span class="hljs-keyword">and</span> less_zero_median &lt; zero_data_median &lt; biger_zero_median:<br>        <span class="hljs-comment">#基本上是单调递增的，那么返回0的附近的对应的原始数据的均值, 即zero_data_mean</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>, zero_data_mean, feature_name<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>, <span class="hljs-number">0</span>, feature_name<br><br><span class="hljs-comment"># 打印前10个特征，按照shap值的重要性排序</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>):<br>    mean_shape = shap_values[:, shap_values.<span class="hljs-built_in">abs</span>.mean(<span class="hljs-number">0</span>).argsort[-i]]<br>    is_monotone, middle_data, feature_name = get_middle_data(mean_shape)<br>    fig = plt.gcf()<br>    fig.set_size_inches(<span class="hljs-number">18.5</span>, <span class="hljs-number">10.5</span>, forward=<span class="hljs-literal">True</span>)<br>    ax = fig.gca()<br>    <span class="hljs-keyword">if</span> is_monotone:<br>        title = <span class="hljs-string">f&quot;特征<span class="hljs-subst">&#123;i&#125;</span>_<span class="hljs-subst">&#123;feature_name&#125;</span>是基本上是单调递增的，对应的shap值0附近的原始特征数据均值值是:<span class="hljs-subst">&#123;middle_data&#125;</span>&quot;</span><br>    <span class="hljs-keyword">else</span>:<br>        title = <span class="hljs-string">f&quot;特征<span class="hljs-subst">&#123;i&#125;</span>_<span class="hljs-subst">&#123;feature_name&#125;</span>不是单调递增的&quot;</span><br>    ax.set_title(title)<br>    shap.plots.scatter(shap_values = mean_shape, ax=ax)<br></code></pre></td></tr></table></figure><h1 id="绘图结果，按照特征重要性进行的排序"><a href="#绘图结果，按照特征重要性进行的排序" class="headerlink" title="绘图结果，按照特征重要性进行的排序"></a>绘图结果，按照特征重要性进行的排序</h1><img src="/2022/02/18/shap-explore2/shap1.png" class=""><img src="/2022/02/18/shap-explore2/shap2.png" class=""><img src="/2022/02/18/shap-explore2/shap3.png" class=""><img src="/2022/02/18/shap-explore2/shap4.png" class=""><img src="/2022/02/18/shap-explore2/shap5.png" class=""><img src="/2022/02/18/shap-explore2/shap6.png" class=""><img src="/2022/02/18/shap-explore2/shap7.png" class=""><img src="/2022/02/18/shap-explore2/shap8.png" class=""><img src="/2022/02/18/shap-explore2/shap9.png" class="">]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
