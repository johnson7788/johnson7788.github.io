

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/starfish.webp">
  <link rel="icon" href="/img/starfish.webp">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Johnson">
  <meta name="keywords" content="">
  
    <meta name="description" content="一、引言如下分享的目的主要是为了拓展AIGC方面的思路和视野，谁也没想到最先应用的计算机视觉方向居然被自然语言处理抢了风头，例如目标检测方向的人脸识别和聊天模型Chatgpt，不过未来的模型一定向合并的趋势，即智能的多模态方向发展。 介绍AIGC，介绍视觉，介绍多模态： AIGC就是人工智能生成内容（Artificial Intelligence Generative Content），也就是让A">
<meta property="og:type" content="article">
<meta property="og:title" content="AIGC之视觉和多模态">
<meta property="og:url" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/index.html">
<meta property="og:site_name" content="Be With you">
<meta property="og:description" content="一、引言如下分享的目的主要是为了拓展AIGC方面的思路和视野，谁也没想到最先应用的计算机视觉方向居然被自然语言处理抢了风头，例如目标检测方向的人脸识别和聊天模型Chatgpt，不过未来的模型一定向合并的趋势，即智能的多模态方向发展。 介绍AIGC，介绍视觉，介绍多模态： AIGC就是人工智能生成内容（Artificial Intelligence Generative Content），也就是让A">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/upscaly.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/upscale.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/codeformer.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image-20231030134601462.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/inpainting.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image_editing.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/ir.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/ski.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image_edit.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/bird_3d.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/pvd_teaser.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/I2VGen-XL.gif">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/GAN-8635689.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/Forward_diffusion.webp">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/stable_diffu.webp">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/reversed_stable.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/VAE.webp">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/Unet.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/stable-quan.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/text_prompt.webp">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/NeRF_dynVideo.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image-20231101113414034.png">
<meta property="og:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/nextgpt.png">
<meta property="article:published_time" content="2023-11-07T00:58:01.000Z">
<meta property="article:modified_time" content="2023-11-09T05:10:45.385Z">
<meta property="article:author" content="Johnson">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/upscaly.gif">
  
  
  <title>AIGC之视觉和多模态 - Be With you</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"johnson7788.github.io","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>分享</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          <span id="subtitle" title="AIGC之视觉和多模态">
            
          </span>
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-07 08:58" pubdate>
          2023年11月7日 早上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          66 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">AIGC之视觉和多模态</h1>
            
            <div class="markdown-body">
              
              <h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>如下分享的目的主要是为了拓展AIGC方面的思路和视野，谁也没想到最先应用的计算机视觉方向居然被自然语言处理抢了风头，例如目标检测方向的人脸识别和聊天模型Chatgpt，不过未来的模型一定向合并的趋势，即智能的多模态方向发展。</p>
<p>介绍AIGC，介绍视觉，介绍多模态：</p>
<p>AIGC就是人工智能生成内容（Artificial Intelligence Generative Content），也就是让AI自己动手创作各种各样的内容，比如图片、视频、音乐、文字等等。AI：机器模拟人，能够做到看、听、说、想、做。GC：Generated Content，类似用户生成内容（UGC），到专业生成内容（PGC），现在是AIGC，例如，ChatGPT，stable diffusion。</p>
<p>AIGC的视觉代表技术DALL·E，Midjourney，stable diffusion，主要利用扩散模型进行文本生成图像或图像生成图像的技术。</p>
<p>多模态代表多种信号的互相生成和转换，例如根据文本生成音频，视频。给定视频，生成文本摘要信息，根据要求修改视频等。</p>
<h1 id="二、AIGC在视觉任务中的原理和应用"><a href="#二、AIGC在视觉任务中的原理和应用" class="headerlink" title="二、AIGC在视觉任务中的原理和应用"></a>二、AIGC在视觉任务中的原理和应用</h1><h2 id="2-1-AIGC-NLP"><a href="#2-1-AIGC-NLP" class="headerlink" title="2.1 AIGC-NLP"></a>2.1 <strong>AIGC-NLP</strong></h2><p>1）大语言模型LLM用于问答，闲聊和专业问答。</p>
<p>2）用于代码生成，辅助写代码。</p>
<p>发展方向：大模型向Agent的方向和多模态的方向发展了，更智能的话就现在除了堆参数外，模型结构上暂无突破。</p>
<h2 id="2-2-AIGC-CV"><a href="#2-2-AIGC-CV" class="headerlink" title="2.2 AIGC-CV"></a>2.2 AIGC-CV</h2><p>1.图片超分辨率: Image Super Resolution</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/upscaly.gif" srcset="/img/loading.gif" lazyload class="">
<p>支持多种放大算法： <a target="_blank" rel="noopener" href="https://www.upscayl.org/">https://www.upscayl.org/</a></p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/upscale.png" srcset="/img/loading.gif" lazyload class="">
<p>2.上海AI实验室： <a target="_blank" rel="noopener" href="https://github.com/XPixelGroup/DiffBIR">利用生成扩散先验实现盲图像恢复</a></p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/codeformer.png" srcset="/img/loading.gif" lazyload class="">

<p>3.针对面部的高清修复： <a target="_blank" rel="noopener" href="https://replicate.com/sczhou/codeformer">sczhou&#x2F;codeformer – Run with an API on Replicate</a></p>
<p>4.针对面部的高清修复, 腾讯GFPGAN</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image-20231030134601462.png" srcset="/img/loading.gif" lazyload class="">

<p><a target="_blank" rel="noopener" href="https://github.com/TencentARC/GFPGAN">TencentARC&#x2F;GFPGAN: GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration. (github.com)</a></p>
<p>5.图像修复Inpainting和Outpainting</p>
<p>Outpainting是一种利用人工智能技术生成新像素的技术，在不破坏图像原有边界的情况下，无缝地扩展图像。这意味着可以向图像中添加新的细节、扩展背景或创建全景视图，而不会出现明显的接缝或瑕疵。Outpainting技术通过生成视觉连贯和逼真的像素扩展，使图像的构图更加丰富或创造更广阔的视觉背景。</p>
<p>Inpainting是一种利用人工智能技术填补图像缺失部分的技术。当图像中存在缺失、损坏或遮挡时，Inpainting技术可以根据周围的图像信息进行推测和重建，以填补缺失的区域。其目标是使修复后的图像在视觉上保持连贯和自然，使缺失部分与周围环境融合无缝，让人难以察觉修复的痕迹。Inpainting技术在图像修复、恢复古老照片、去除不需要的对象等领域具有广泛的应用。</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/inpainting.gif" srcset="/img/loading.gif" lazyload class="">

<p><video src="../../../Users/admin/Movies/notes/teaser_boy_compressed.mp4"></video></p>
<p>6.生成修复模型，使用场景的一些参考图像即可实现个性化修复，生成忠实于原始场景的新图：<a target="_blank" rel="noopener" href="https://realfill.github.io/">RealFill</a></p>
<p>7.<strong>image editing，根据文字对图片进行修改</strong></p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image_editing.png" srcset="/img/loading.gif" lazyload class="">

<p><a target="_blank" rel="noopener" href="https://imagic-editing.github.io/#">Imagic: Text-Based Real Image Editing with Diffusion Models (imagic-editing.github.io)</a></p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/ir.gif" srcset="/img/loading.gif" lazyload class="">

<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/ski.gif" srcset="/img/loading.gif" lazyload class="">

<p>8.根据提供的参考图像，对目标图像进行编辑。包括物品，人物的外貌修改，形状修改，动作修改等。Picsart-AI-Research&#x2F;PAIR-Diffusion: PAIR Diffusion: A Comprehensive Multimodal Object-Level Image Editor, 2023 (github.com)](<a target="_blank" rel="noopener" href="https://github.com/Picsart-AI-Research/PAIR-Diffusion">https://github.com/Picsart-AI-Research/PAIR-Diffusion</a>)</p>
<p>修改图像中的物品</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image_edit.gif" srcset="/img/loading.gif" lazyload class="">

<p>9.零样本，适应不同场景，保持纹理细节，同时允许多种局部变化（例如，照明、方向、姿势等），支持对象与不同环境的良好融合。<a target="_blank" rel="noopener" href="https://damo-vilab.github.io/AnyDoor-Page/">AnyDoor (damo-vilab.github.io)</a></p>
<h2 id="2-3-AIGC-3D"><a href="#2-3-AIGC-3D" class="headerlink" title="2.3 AIGC-3D"></a>2.3 <strong>AIGC-3D</strong></h2><p>总结：较为初级。</p>
<p>1.英伟达Magic3D：<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dir/magic3d/">Magic3D: High-Resolution Text-to-3D Content Creation (nvidia.com)</a></p>
<p><video src="/Users/admin/Movies/notes/Magic3Dvideo.mp4"></video></p>
<p>2.Dream Fusion:</p>
<p><a target="_blank" rel="noopener" href="https://dreamfusion3d.github.io/">DreamFusion: Text-to-3D using 2D Diffusion (dreamfusion3d.github.io)</a></p>
<p>一只读书的松鼠:</p>
<p><video src="../../../Users/admin/Movies/notes/a_DSLR_photo_of_a_squirrel__reading_a_book_rgbdn_hq_15000.mp4"></video></p>
<p>3.Shape-E:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/shap-e">openai&#x2F;shap-e: Generate 3D objects conditioned on text or images (github.com)</a></p>
<p>生成1只鸟：A bird</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/bird_3d.gif" srcset="/img/loading.gif" lazyload class="">

<p>4.Point-Voxel Diffusion： 点体素扩散（PVD）生成点云，点云在创建逼真的三维模型、场景和动画很有用，对于电影、游戏开发、虚拟现实和增强现实等领域非常重要。在重建真实世界中的物体，自动驾驶和机器人领域也被广泛应用，激光雷达可以生成高分辨率的点云地图，帮助自动驾驶车辆感知和理解周围环境。机器人可以利用点云数据进行环境感知、障碍物检测和路径规划等任务。</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/pvd_teaser.gif" srcset="/img/loading.gif" lazyload class="">

<p>5.<a target="_blank" rel="noopener" href="https://alexzhou907.github.io/pvd">Linqi (Alex) Zhou — Linqi (Alex) Zhou (alexzhou907.github.io)</a></p>
<p><video src="../../../Users/admin/Movies/notes/HOSNeRF__Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video.mp4"></video><br>6.3D场景渲染，只要根据一个视频就能随时360度渲染其它视觉。新加坡视觉实验室和腾讯开发：<a target="_blank" rel="noopener" href="https://showlab.github.io/HOSNeRF/">HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video (showlab.github.io)</a></p>
<p><video src="../../../Users/admin/Movies/notes/img_pearl.mp4"></video></p>
<p>7.OPPO研发的高精度3D重建。<a target="_blank" rel="noopener" href="https://oppo-us-research.github.io/NeuRBF-website/">NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions (oppo-us-research.github.io)</a></p>
<p><video src="../../../Users/admin/Movies/notes/neuman_citron_54759e750c.mp4"></video></p>
<p>8.苹果研发的增强现实，苹果的VR设备，人物背景视频合成</p>
<p>更多资料:</p>
<p><a target="_blank" rel="noopener" href="https://sweetdreamer3d.github.io/">SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D — SweetDreamer：在 2D 扩散中对齐几何先验以实现一致的文本到 3D (sweetdreamer3d.github.io)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.xxlong.site/Wonder3D/">Wonder3D: Single Image to 3D using Cross-Domain Diffusion (xxlong.site)</a></p>
<p><a target="_blank" rel="noopener" href="https://mrtornado24.github.io/DreamCraft3D/">https://mrtornado24.github.io/DreamCraft3D/</a></p>
<h2 id="2-4-AIGC-Video"><a href="#2-4-AIGC-Video" class="headerlink" title="2.4 AIGC-Video"></a>2.4 AIGC-Video</h2><p>视频扩散模型(VLDM)</p>
<p>1.图片动起来，从真实视频序列中提取的运动轨迹集合中学习到的模型，例如树木、花朵、蜡烛和在风中摇曳的衣服。2种方式，无缝循环和交互式互动。: <a target="_blank" rel="noopener" href="https://generative-dynamics.github.io/">Generative Image Dynamics (generative-dynamics.github.io)</a></p>
<p><video src="../../../Users/admin/Movies/notes/teaser_new.mp4"></video></p>
<p>2.阿里I2VGen-XL，视频扩散模型(VLDM)：<a target="_blank" rel="noopener" href="https://modelscope.cn/models/damo/Image-to-Video/summary">https://modelscope.cn/models/damo/Image-to-Video/summary</a></p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/I2VGen-XL.gif" srcset="/img/loading.gif" lazyload class="">

<p>3.<a target="_blank" rel="noopener" href="https://animatediff.github.io/">AnimateDiff</a></p>
<p><video src="../../../Users/admin/Movies/notes/ani_01.mp4"></video></p>
<p><video src="../../../Users/admin/Movies/notes/ani_05.mp4"></video></p>
<p>大海，提示如下:</p>
<p>photo of coastline, rocks, storm weather, wind, waves, lightning, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3</p>
<p><video src="../../../Users/admin/Movies/notes/sea.mp4"></video></p>
<p>4.视频动态修改</p>
<p><video src="../../../Users/admin/Movies/notes/input_fps30.mp4"></video></p>
<p><video src="../../../Users/admin/Movies/notes/result_fps_30.mp4"></video></p>
<p>根据原始视频和文字对视频进行修改，难点在于视频中事物的时间一致性。</p>
<p><a target="_blank" rel="noopener" href="https://diffusion-tokenflow.github.io/">TokenFlow: Consistent Diffusion Features for Consistent Video Editing (diffusion-tokenflow.github.io)</a></p>
<p><video src="../../../Users/admin/Movies/notes/AIGen-24K.mp4"></video></p>
<p><video src="../../../Users/admin/Movies/notes/rungen2.mov"></video></p>
<p>5.效果很好的商业项目：Gen2: <a target="_blank" rel="noopener" href="https://research.runwayml.com/gen2">Gen-2 by Runway — 跑道 Gen-2 (runwayml.com)</a></p>
<p><video src="../../../Users/admin/Movies/notes/ProPainter__Improving_Propagation_and_Transformer_for_Video_Inpainting_(ICCV_2023).mp4"></video></p>
<p>6.视频物体去除：<a target="_blank" rel="noopener" href="https://shangchenzhou.com/projects/ProPainter/">ProPainter for Video Inpainting (shangchenzhou.com)</a></p>
<p>7.动作捕捉和替换（商业版)</p>
<p><video src="../../../Users/admin/Movies/notes/motion.mp4"></video></p>
<p><a target="_blank" rel="noopener" href="https://www.move.ai/">https://www.move.ai/</a></p>
<p>8.视频人物替换（商业版)<a target="_blank" rel="noopener" href="https://wonderdynamics.com/">https://wonderdynamics.com/</a></p>
<p><video src="../../../Users/admin/Movies/notes/wonder_studio_(360p).mp4"></video></p>
<p>9.人物修改(开源版),  可以根据图片，视频，文字，生成新的视频</p>
<p><video src="../../../Users/admin/Movies/notes/sample11_blur.mp4"></video></p>
<p><video src="../../../Users/admin/Movies/notes/Magicavatar_Introduction_Video.mp4"></video></p>
<p><a target="_blank" rel="noopener" href="https://magic-avatar.github.io/">MagicAvatar: Multi-modal Avatar Generation and Animation (magic-avatar.github.io)</a></p>
<p>10.人物和场景修改</p>
<p><video src="../../../Users/admin/Movies/notes/video_edit.mov"></video></p>
<p><a target="_blank" rel="noopener" href="https://magic-edit.github.io/">MagicEdit: High-Fidelity Temporally Coherent Video Editing (magic-edit.github.io)</a></p>
<p>11.视频中的场景和人物根据参考背景和人物进行替换，下面有具体原理讲解：</p>
<p><video src="../../../Users/admin/Movies/notes/DynVideo-E_Demo_Video.mp4"></video></p>
<p><a target="_blank" rel="noopener" href="https://showlab.github.io/DynVideo-E/">DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing (showlab.github.io)</a></p>
<p>12.视频的控制生成</p>
<p><video src="../../../Users/admin/Movies/notes/control_video.mp4"></video></p>
<p><a target="_blank" rel="noopener" href="https://twitter.com/blockadelabs">Blockade Labs (@BlockadeLabs) &#x2F; X (twitter.com)</a></p>
<h2 id="2-5-Stable-diffusion-原理"><a href="#2-5-Stable-diffusion-原理" class="headerlink" title="2.5 Stable diffusion 原理"></a>2.5 Stable diffusion 原理</h2><p><video src="../../../Users/admin/Movies/notes/stable_diffusion_video.mp4"></video></p>
<p>GAN–&gt;扩散模型–&gt;潜空间扩散模型</p>
<h3 id="2-5-1-GAN：-生成对抗网络，Generative-Adversarial-Networks-最初的视觉生成模型。"><a href="#2-5-1-GAN：-生成对抗网络，Generative-Adversarial-Networks-最初的视觉生成模型。" class="headerlink" title="2.5.1 GAN： 生成对抗网络，Generative Adversarial Networks, 最初的视觉生成模型。"></a><strong>2.5.1 GAN：</strong> 生成对抗网络，Generative Adversarial Networks, 最初的视觉生成模型。</h3><img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/GAN-8635689.png" srcset="/img/loading.gif" lazyload class="">

<ol>
<li>定义一个模型来作为生成器（图中蓝色部分Generator），能够输入一个向量，输出手写数字大小的像素图像。</li>
<li>定义一个分类器来作为判别器（图中红色部分Discriminator）用来判别图片是真的还是假的（或者说是来自数据集中的还是生成器中生成的），输入为手写图片，输出为判别图片的标签。</li>
</ol>
<p>生成对抗网络具有如下优缺点：</p>
<ul>
<li>生成对抗网络的生成模型可以生成高质量、逼真的图像、音频等内容；</li>
<li>与使用马尔可夫链反复采样的传统生成模型相比，生成对抗网络不需要反复采样，因此它们可以更加高效地处理高维数据；</li>
<li>可以采用任何形式的网络结构来学习数据分布的映射关系，无需遵循因式分解模型；</li>
<li><strong>训练过程不稳定，容易发生模式崩溃</strong>、模式震荡等问题，造成训练中断；</li>
<li>由于缺乏预先建模的约束，因此生成的样本可能会出现一些意外的、<strong>不可控</strong>的情况。</li>
</ul>
<h3 id="2-5-2-扩散模型原理"><a href="#2-5-2-扩散模型原理" class="headerlink" title="2.5.2 扩散模型原理:"></a><strong>2.5.2 扩散模型原理:</strong></h3><p>前向扩散过程将噪声添加到训练图像，逐渐将其变成非特征噪声图像。前向处理将把任何猫或狗图像变成噪声图像。反向过程是从噪声图像恢复出真实图像。</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/Forward_diffusion.webp" srcset="/img/loading.gif" lazyload class="">

<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/stable_diffu.webp" srcset="/img/loading.gif" lazyload class="">

<p>要实现反向扩散，首先需要知道有多少噪声被添加到图像中，前向过程中让模型学习添加的噪声。</p>
<p>1）选择一个训练图像，比如一张猫的照片。</p>
<p>2）生成随机噪声图像。</p>
<p>3）将此噪声图像添加到猫的图片中。</p>
<p>4）噪声预测模型预测添加了多少噪声。</p>
<p>反向过程使用噪声预测模型</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/reversed_stable.png" srcset="/img/loading.gif" lazyload class="">

<p>首先生成一个完全随机的图像，并要求噪声预测模型告诉我们噪声。然后我们从原始图像中减去这个估计的噪声。重复这个过程几次。你会得到一个猫或狗的图像。</p>
<p>缺点：不稳定，速度非常慢，需要庞大的step。</p>
<h3 id="2-5-3-VAE-Variational-Autoencoder-变分自编码器"><a href="#2-5-3-VAE-Variational-Autoencoder-变分自编码器" class="headerlink" title="2.5.3 VAE Variational Autoencoder 变分自编码器"></a><strong>2.5.3 VAE Variational Autoencoder 变分自编码器</strong></h3><p>（1）编码器和（2）解码器。编码器将图像压缩为潜在空间中的较低维表示。解码器从潜在空间恢复图像。</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/VAE.webp" srcset="/img/loading.gif" lazyload class="">

<h3 id="2-5-4-Latent-diffusion-model-潜扩散模型"><a href="#2-5-4-Latent-diffusion-model-潜扩散模型" class="headerlink" title="2.5.4 Latent diffusion model 潜扩散模型"></a><strong>2.5.4 Latent diffusion model 潜扩散模型</strong></h3><p>利用VAE的特性，对潜空间进行正向和反向扩散的过程。</p>
<p>1）生成随机潜在空间矩阵。</p>
<p>2）噪声预测模型估计潜在矩阵的噪声。</p>
<p>3）然后从潜在矩阵中减去估计的噪声。</p>
<p>4）重复步骤2和3，直到特定的采样步骤。</p>
<p>Tips：</p>
<p>SDXL默认训练图片尺寸是1024x1024， SD1.5是512x512</p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://stable-diffusion-art.com/how-stable-diffusion-work/">https://stable-diffusion-art.com/how-stable-diffusion-work/</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.00796">https://arxiv.org/abs/2209.00796</a></p>
<h3 id="2-5-5-UNet-噪声预测模型"><a href="#2-5-5-UNet-噪声预测模型" class="headerlink" title="2.5.5 UNet, 噪声预测模型"></a><strong>2.5.5 UNet, 噪声预测模型</strong></h3><img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/Unet.png" srcset="/img/loading.gif" lazyload class="">

<p>和自动编码器类似，区别是自动编码器的编码器和解码器可以独立使用。但在 UNet 中，解码器与编码器的跳跃连接相连，使用concat加法，编码器在每个上采样步骤为解码器提供有用的信息。</p>
<p>完整的stable-diffusion结构:</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/stable-quan.png" srcset="/img/loading.gif" lazyload class="">

<h3 id="2-5-6-Text-conditioning-diffusion-Model-文本条件扩散模型"><a href="#2-5-6-Text-conditioning-diffusion-Model-文本条件扩散模型" class="headerlink" title="2.5.6 Text conditioning diffusion Model 文本条件扩散模型"></a><strong>2.5.6 Text conditioning diffusion Model 文本条件扩散模型</strong></h3><p>CLIP模型：Openai的多模态模型，图像文本对训练，可以图像生成文本描述。这部分生成文本的向量所以是由CLIP模型负责，现在用更好的OpenCLIP代替。</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/text_prompt.webp" srcset="/img/loading.gif" lazyload class="">

<h2 id="2-6-DynVideo-E-视频修改原理"><a href="#2-6-DynVideo-E-视频修改原理" class="headerlink" title="2.6 DynVideo-E 视频修改原理"></a>2.6 DynVideo-E 视频修改原理</h2><p>思考：</p>
<p>有3个要点，1个是背景修改，2是人物修改，3是动作要保持一致</p>
<p>创新点：</p>
<ul>
<li>提出video-NeRF表示来建模动态人体和静态背景</li>
<li>使用三维和二维分数蒸馏采样(SDS)来提高三维一致性</li>
<li>基于文本的超分辨率放大关键身体区域</li>
<li>采用图像风格迁移来编辑三维背景(结构图最上面)</li>
</ul>
<p>缺点：</p>
<p>该方法只适用于以人为中心的动态视频，如果场景中没有人体,则无法直接应用。后续工作可以考虑扩展到更一般的动态场景。</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/NeRF_dynVideo.png" srcset="/img/loading.gif" lazyload class="">

<p>NeRF:Neural Radiance Fields 神经辐射场，NeRF 的核心思想是将场景表示为神经辐射场，它将三维空间中的每个点映射到颜色和密度值。传统的渲染方法在三维场景中需要建立复杂的模型和纹理映射，而 NeRF 通过学习从观察视角到场景颜色和密度之间的映射关系，可以以更少的数据和先验知识来生成逼真的渲染结果。</p>
<p>$$<br>F_\theta(\boldsymbol{x}, \boldsymbol{d})&#x3D;[\sigma, \boldsymbol{c}]<br>$$<br>x是空间的坐标x,y,z。d是观察方向，sigma是密度，这里的密度值代表的是光线在改点的终止的概率。c是该点的颜色RGB。NeRF的输入：要生成图像上某点的3D坐标(x,y,z)，和观看者的角度, NeRF输出该点的颜色和透明度。</p>
<p>🤔<strong>其实所有的生成神经网络都是一个范式，即压缩和还原，压缩是学习事物，学习一种泛化性，压缩到一个潜空间，还原是生成一个原事物或新事物。</strong></p>
<p>SDS: Score Distillation Sampling 分数蒸馏采样 : 由于构建神经半径场（NeRF）所需的数据有限，研究人员开始探索蒸馏方法，以获得与给定文本提示相关的NeRF。Poole等人的开创性工作 提出了一种称为分数蒸馏采样（SDS）的方法，将2D文本到图像扩散模型蒸馏成3D NeRF。与传统的NeRF构建方法不同，传统方法需要目标3D物体的多个视角图像，而基于文本的NeRF构建缺少了3D物体和多个视角图像。<strong>SDS方法通过最小化固定视角下NeRF生成的图像与扩散模型的损失函数来优化NeRF</strong>。为了避免直接优化扩散模型的损失函数所带来的计算开销，研究人员提出通过省略Unet雅可比项来近似蒸馏目标。</p>
<h1 id="三、AIGC在多模态任务中的原理和应用"><a href="#三、AIGC在多模态任务中的原理和应用" class="headerlink" title="三、AIGC在多模态任务中的原理和应用"></a>三、AIGC在多模态任务中的原理和应用</h1><p>介绍多模态问答，声音生成</p>
<h2 id="3-1-AIGC-QA-多模态问答"><a href="#3-1-AIGC-QA-多模态问答" class="headerlink" title="3.1 AIGC-QA 多模态问答"></a>3.1 AIGC-QA 多模态问答</h2><p><video src="../../../Users/admin/Movies/notes/Next-GPT.mp4"></video></p>
<p><a target="_blank" rel="noopener" href="https://github.com/NExT-GPT/NExT-GPT">NExT-GPT&#x2F;NExT-GPT: Code and models for NExT-GPT: Any-to-Any Multimodal Large Language Model (github.com)</a></p>
<p><video src="../../../Users/admin/Movies/notes/llava.mov"></video><br><a target="_blank" rel="noopener" href="https://llava-vl.github.io/">https://llava-vl.github.io/</a></p>
<p>GPT-4 (all-Tools)</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/image-20231101113414034.png" srcset="/img/loading.gif" lazyload class="">

<p>更多资料:</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.02239.pdf">arxiv.org&#x2F;pdf&#x2F;2310.02239.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://dediffusion.github.io/">De-Diffusion (dediffusion.github.io)</a></p>
<h2 id="3-2-AIGC-Audio"><a href="#3-2-AIGC-Audio" class="headerlink" title="3.2 AIGC-Audio"></a>3.2 <strong>AIGC-Audio</strong></h2><p>Meta的seamless-m4t: <a target="_blank" rel="noopener" href="https://ai.meta.com/blog/seamless-m4t/">Introducing a foundational multimodal model for speech translation — 引入语音翻译的基础多模态模型 (meta.com)</a></p>
<p><video src="../../../Users/admin/Movies/notes/seamless4.mp4"></video></p>
<p>视频声音翻译加音色转换</p>
<p><video src="../../../Users/admin/Movies/notes/guodegang.mp4"></video></p>
<p><video src="/Users/admin/Movies/notes/hgenv2.mp4"></video></p>
<p><a target="_blank" rel="noopener" href="https://labs.heygen.com/guest/video-translate">HeyGen - AI Spokesperson Video Creator</a></p>
<p><video src="../../../Users/admin/Movies/notes/Cosmos___AI_Dubbing_Product_Demo.mp4"></video></p>
<p>类似HeyGen的工具, <a target="_blank" rel="noopener" href="https://elevenlabs.io/dubbing">AI Dubbing: Translate Video and Voice with the Best Voiceover Tool (elevenlabs.io)</a></p>
<p>生成音乐: <a target="_blank" rel="noopener" href="https://www.stableaudio.com/">Stable Audio - Generative AI for music &amp; sound fx</a><br>Trance, Ibiza, Beach, Sun, 4 AM, Progressive, Synthesizer, 909, Dramatic Chords, Choir, Euphoric, Nostalgic, Dynamic, Flowing</p>
<p><audio src="../../../Users/admin/Movies/notes/trance-ibiza-beach-sun-4-am-progressive-synthesizer-909-dramatic-chords-choir-euphoric-nostalgic-dynamic-flowing.mp3"></audio></p>
<h2 id="3-3-Next-GPT原理"><a href="#3-3-Next-GPT原理" class="headerlink" title="3.3 Next-GPT原理"></a>3.3 Next-GPT原理</h2><p>作用：模拟人的多模态感知和输出能力,超越GPT4的多模态聊天。<br>优点：接收文本，图像，视频，音频的任意组合，并输出它们，只微调已训练过模型的投影层，训练量小，扩展性高，灵活更换更多模型和模态<br>缺点：中文一般，有时不能按指令生成图片，音频和视频，可能模态指令转换数据集不全面引起的，导致不能生成其它模态的token<br>原理: 以大语言模型为核心，不同模态的编码器作为输入，解码器作为输出。中间用投影层作为胶水黏合，只需要对投影层进行微调</p>
<p>结构:<br><strong>编码阶段:</strong><br>        利用现有的各个模态的编码器对各种模态的输入进行编码，然后通过投影层将这些表示投影到 LLM 可理解的类语言表示中。<br>        使用Meta的ImageBind模型<br><strong>LLM理解和推理阶段:</strong><br>        利用现有的开源 LLM 作为核心，处理输入信息以进行语义理解和推理。LLM 不仅能直接生成文本token，还能生成独特的 “模态信号 “token，作为指令表明解码层是否要输出相应的模态内容<br>        使用Vicuna2<br>        输出<br>            1）直接的文本响应<br>            2）每种模态的信号token<br>                这些信号token作为指令表明解码层是否生成多模态内容，以及如果生成则生成什么内容。<br><strong>解码阶段:</strong><br>        产生的带有特定指令的多模态信号经过投射后，会进入不同的编码器，最终生成相应模态的内容。<br>        图像模型stable diffusion,视频合成的 Zeroscope，音频合成的 AudioLDM</p>
<img src="/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/nextgpt.png" srcset="/img/loading.gif" lazyload class="">

<h1 id="四、在线试用"><a href="#四、在线试用" class="headerlink" title="四、在线试用"></a>四、在线试用</h1><p>聊天:</p>
<p><a target="_blank" rel="noopener" href="https://chatglm.cn/main/detail">https://chatglm.cn/main/detail</a></p>
<p><a target="_blank" rel="noopener" href="https://www.baichuan-ai.com/home">https://www.baichuan-ai.com/home</a></p>
<p><a target="_blank" rel="noopener" href="https://yiyan.baidu.com/welcome">https://yiyan.baidu.com/welcome</a></p>
<p>文生图:</p>
<p><a target="_blank" rel="noopener" href="https://jsai.cc/ai-muses/gallery">https://jsai.cc/ai-muses/gallery</a></p>
<p><a target="_blank" rel="noopener" href="https://app.runwayml.com/">https://app.runwayml.com/</a></p>
<h1 id="五、结论"><a href="#五、结论" class="headerlink" title="五、结论"></a>五、结论</h1><p>AIGC在视觉和多模态方向经历了从GAN到VAE再到扩散模型的发展过程。在视觉方面,GAN的生成效果好但训练不稳定,扩散模型克服了这一缺点,得到了更好的生成效果。随着VAE的引入,扩散模型在潜空间中进行,生成更稳定。文本提示的引入使得扩散模型可以进行条件图像生成。在多模态方面,大语言模型的应用让不同模态之间的转换成为可能,诸如图片到文本,文本到语音等。下一步的挑战是实现真正的多模态一体化,不同模态之间无缝转换。在视觉方面,需要处理更复杂的三维场景;在多模态方面,需要处理不同模态之间的语义一致性。未来预计会看到基于transformer的统一多模态模型的出现,实现任意模态之间的转换。总体来说,AIGC在视觉和多模态方向拓展了计算机的感知和表达能力,朝着模拟人类智能的方向前进,但仍需进一步的研究来克服现有的局限,实现真正的通用人工智能。</p>
<p>AIGC的多模态未来充满想象空间，随着计算能力、数据和算法的发展，我觉得超越人类的智能不远了。</p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>AIGC之视觉和多模态</div>
      <div>https://johnson7788.github.io/2023/11/07/AIGC%E4%B9%8B%E8%A7%86%E8%A7%89%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Johnson</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年11月7日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/11/16/%E9%A6%99%E6%B0%9Bchat%E7%9A%84%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%B7%A5%E4%BD%9C/" title="香氛chat的数据收集工作">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">香氛chat的数据收集工作</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/11/01/autoagent%E5%AE%9E%E7%8E%B0%E6%99%BA%E8%83%BD%E9%97%AE%E5%8D%B7%E8%B0%83%E6%9F%A5/" title="autoagent实现智能问卷调查">
                        <span class="hidden-mobile">autoagent实现智能问卷调查</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  
    <script  src="/js/img-lazyload.js" ></script>
  



  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var title = subtitle.title;
      
        typing(title);
      
    })(window, document);
  </script>







  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
